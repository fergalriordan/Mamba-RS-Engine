# Learning Resources

## Foundation Refresher
* [3Blue1Brown: Neural Networks](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
* [Andrej Karpathy's CS231n: Convolutional Neural Networks](https://cs231n.github.io/convolutional-networks/)
* [Residual Blocks](https://arxiv.org/abs/1512.03385)
* [Andrej Karpathy: The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)
* [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
* [RNNs and Attention](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)

## Transformers
* [Illustrated Transformers (Transformers and Self Attention)](https://jalammar.github.io/illustrated-transformer/)
* [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)
* [Andrej Karpathy: Let's Build GPT From Scratch](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)

## Vision Transformers
* [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)
* [Illustrated Vision Tranformers](https://medium.com/analytics-vidhya/illustrated-vision-transformers-165f4d0c3dd1)

## Mamba and SSMs
* [A Visual Guide to Mamba and Space State Models: An Alternative to Transformers for Language Modelling](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state)
* [Introduction to Space State Models as Natural Language Models](https://neptune.ai/blog/state-space-models-as-natural-language-models)

## Visual Mamba

* [VMamba: Visual State Space Model](https://arxiv.org/abs/2401.10166)
